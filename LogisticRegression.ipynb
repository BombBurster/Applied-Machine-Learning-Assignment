{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absenteeism - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 740 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason for absence</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation expense</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>Hit target</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Absenteeism time in hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692552</td>\n",
       "      <td>0.724379</td>\n",
       "      <td>0.312360</td>\n",
       "      <td>0.337461</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>0.463951</td>\n",
       "      <td>0.610497</td>\n",
       "      <td>0.647262</td>\n",
       "      <td>0.653058</td>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.360488</td>\n",
       "      <td>0.253737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.124304</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.378506</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>0.677741</td>\n",
       "      <td>0.127401</td>\n",
       "      <td>0.945206</td>\n",
       "      <td>0.872130</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.354537</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.432080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369064</td>\n",
       "      <td>0.337820</td>\n",
       "      <td>0.083777</td>\n",
       "      <td>0.678456</td>\n",
       "      <td>0.703634</td>\n",
       "      <td>0.173419</td>\n",
       "      <td>0.744381</td>\n",
       "      <td>0.702182</td>\n",
       "      <td>0.920467</td>\n",
       "      <td>0.876997</td>\n",
       "      <td>0.690698</td>\n",
       "      <td>0.470167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.739270</td>\n",
       "      <td>0.646078</td>\n",
       "      <td>0.617184</td>\n",
       "      <td>0.416416</td>\n",
       "      <td>0.523913</td>\n",
       "      <td>0.448263</td>\n",
       "      <td>0.562862</td>\n",
       "      <td>0.681659</td>\n",
       "      <td>0.352872</td>\n",
       "      <td>0.360298</td>\n",
       "      <td>0.462425</td>\n",
       "      <td>0.249254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697562</td>\n",
       "      <td>0.725001</td>\n",
       "      <td>0.330455</td>\n",
       "      <td>0.338349</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>0.457796</td>\n",
       "      <td>0.604668</td>\n",
       "      <td>0.650673</td>\n",
       "      <td>0.657022</td>\n",
       "      <td>0.678083</td>\n",
       "      <td>0.361873</td>\n",
       "      <td>0.250986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Reason for absence  Month of absence  Day of the week   Seasons  \\\n",
       "0  0.692552            0.724379          0.312360         0.337461  0.509511   \n",
       "1  0.124304            0.088435          0.378506         0.954500  0.677741   \n",
       "2  0.369064            0.337820          0.083777         0.678456  0.703634   \n",
       "3  0.739270            0.646078          0.617184         0.416416  0.523913   \n",
       "4  0.697562            0.725001          0.330455         0.338349  0.561429   \n",
       "\n",
       "   Transportation expense  Distance from Residence to Work       Age  \\\n",
       "0                0.463951                         0.610497  0.647262   \n",
       "1                0.127401                         0.945206  0.872130   \n",
       "2                0.173419                         0.744381  0.702182   \n",
       "3                0.448263                         0.562862  0.681659   \n",
       "4                0.457796                         0.604668  0.650673   \n",
       "\n",
       "   Work load Average/day   Hit target  Disciplinary failure  Education  \\\n",
       "0                0.653058    0.690259              0.360488   0.253737   \n",
       "1                0.509911    0.354537              0.999076   0.432080   \n",
       "2                0.920467    0.876997              0.690698   0.470167   \n",
       "3                0.352872    0.360298              0.462425   0.249254   \n",
       "4                0.657022    0.678083              0.361873   0.250986   \n",
       "\n",
       "   Absenteeism time in hours  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from csv\n",
    "data_df = pd.read_csv('Absenteeism/Absenteeism_at_work_editted_continous_features_target_not_normalised.csv')\n",
    "\n",
    "print(\"Total:\",len(data_df),\"rows.\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly split data into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataframe and select 80% of the dataset for the training set and the remaining 20% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataframe\n",
    "data_df = data_df.sample(frac=1)\n",
    "\n",
    "# Split data into train and test set\n",
    "train_length = int(np.round(len(data_df) * 0.8))  # Train set: 80% of data\n",
    "test_length = len(data_df) - train_length         # Test set: remaining 20%\n",
    "train_df = data_df.head(train_length)\n",
    "test_df = data_df.tail(test_length)\n",
    "\n",
    "# Features and target (last column) of training set\n",
    "X_train = train_df.iloc[:,:-1].to_numpy()\n",
    "y_train = train_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "# Features and target (last column) of test set\n",
    "X_test = test_df.iloc[:,:-1].to_numpy()\n",
    "y_test = test_df.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea to build a Logistic Regressor model is that we need to determine the weights of features to predict an output.\n",
    "\n",
    "Then, in order to obtain a probability (a number between 0 and 1), we use the sigmoid function. With *w* being each feature's weights and *x* beings the features, our prediction *ŷ* will then be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ŷ = \\frac{1}{1+e^{-w^Tx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this implies we need to find the best weight associated with each feature. To do so, we use gradient descent. At each iteration, we compute the cost with given weights (initialised as a an array of zeros) and iterate to find the weights for which the cost is minimal.\n",
    "\n",
    "The cost function is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cost = -ylog(ŷ) - ((1 - y)log(1-ŷ))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost for all the instances in one training iteration can be computed as the mean of each instance's cost.\n",
    "\n",
    "The gradient descent will enable to minimise this value. At each iteration, we update the weights as by subtracting the gradient, which is the derivative of the cost function with respect to the weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{m}X^T(ŷ - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once the model is trained (i.e. the best weights have been found), we compute the prediction by applying the sigmoid function to wᵀx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class, which we initialise with:\n",
    "- learning rate (default value: 0.01)\n",
    "- maximum number of iterations (default value: 500)\n",
    "- boolean to add intercept (True by default)\n",
    "\n",
    "This last parameter enables to add a bias *w₀* to the weights. For example, with *n* being the number of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^Tx = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter, the learning rate, can control the size of the steps in the gradient descent. The higher the learning rate, the bigger the step, which means it will be faster but at the risk of missing the lowest cost value. The lower the learning rate, the slower the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class also contains the sigmoid function and the cost function mentioned earlier, as well as a `fit()` function to train the model, like we previously explained, and `predict()` to make a prediction for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regressor Model\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    # Initialise hyperparameters\n",
    "    def __init__(self, lr=0.01, max_iter=500, add_intercept=True):\n",
    "        self.learning_rate = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.add_intercept = add_intercept\n",
    "    \n",
    "    # Sigmoid function\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Cost function for gradient descent\n",
    "    def __cost_function(self, y, y_pred):\n",
    "        return (-y*log(y_pred) - ((1-y)*log(1-y_pred))).mean()\n",
    "    \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "        \n",
    "        # Initialise weights\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Keep a history of cost values\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # 1/m (m being the number of intances)\n",
    "        OneOverM = 1 / X.shape[1]\n",
    "        \n",
    "        # Iterate\n",
    "        for i in range (self.max_iter):\n",
    "            \n",
    "            # prediction\n",
    "            y_pred = self.__sigmoid(np.dot(X, self.w.T))\n",
    "            \n",
    "            # cost\n",
    "            cost = self.__cost_function(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # gradient vector\n",
    "            gradient = np.dot(X.T, (y_pred - y)) * OneOverM\n",
    "            self.w -= gradient * self.learning_rate\n",
    "            \n",
    "    # Predict an output\n",
    "    def predict(self, X):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "        y_pred = self.__sigmoid(np.dot(X, self.w.T))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multiclass problem (there are three possible absenteeism categories: 0, 1 and 2), we need to build our Logistic Regression model using One vs. Rest (also called One vs. All). The idea is building a regressor for each class, which determines if the instance belongs to this one class or to the rest.\n",
    "\n",
    "Then, we build a model which combines these regressors and for each instance, chooses the class for which the probability is the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, since we will have three models (one for each possible class), we need three target sets.\n",
    "In a \"one vs. rest\" perspective, for every output, if it is the \"one\" class, we set it to 1, if it is the \"rest\", we set it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1vsall = []\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# For each possible class\n",
    "for c in range (nb_classes):\n",
    "    y_one = np.where(y_train == c, 1, 0)\n",
    "    y_1vsall.append(y_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we make a class for our One vs All model. It has the same parameters as the individual regressors. It has a `fit()` and a `predict()` function as well. The model is trained by training an individual regressor for each class. To predict an output, each model will have to compute the probability that the instance belongs to the \"one\" class (versus the \"rest\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One Vs All Model\n",
    "class LogisticRegressorOneVsAll:\n",
    "    \n",
    "    # Initialise hyperparameters\n",
    "    def __init__(self, lr=0.01, max_iter=500, add_intercept=True):\n",
    "        self.learning_rate = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.add_intercept = add_intercept\n",
    "        \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        self.regressors = []\n",
    "\n",
    "        # For each \"one vs. all\" target set\n",
    "        for y_one in y:\n",
    "            # Build a model with target set\n",
    "            model = LogisticRegressor(lr=self.learning_rate,\n",
    "                                      max_iter=self.max_iter,\n",
    "                                      add_intercept=self.add_intercept)\n",
    "            # Train a model and add it to the list of regressors\n",
    "            model.fit(X, y_one)\n",
    "            self.regressors.append(model)\n",
    "        \n",
    "    # Predict an output\n",
    "    def predict(self, X):\n",
    "        final_pred = []\n",
    "        y_pred_1vsall = []\n",
    "\n",
    "        # For each regressor\n",
    "        for model in self.regressors:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_1vsall.append(y_pred)\n",
    "            \n",
    "        # For each instance\n",
    "        for i in range(len(X)):\n",
    "            best_pred = 0\n",
    "            best_target = -1\n",
    "            # Find the best prediction (model with highest probability)\n",
    "            for j in range(len(self.regressors)):\n",
    "                if y_pred_1vsall[j][i] > best_pred:\n",
    "                    best_pred = y_pred_1vsall[j][i]\n",
    "                    best_target = j\n",
    "            # Add best prediction to the final result\n",
    "            final_pred.append(best_target)\n",
    "\n",
    "        # Return final prediction\n",
    "        final_pred = np.asarray(final_pred)\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 100000\n",
    "\n",
    "# One vs. All regressor\n",
    "model_onevsall = LogisticRegressorOneVsAll(max_iter=nb_iterations)\n",
    "model_onevsall.fit(X_train, y_1vsall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category for each instance:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 0 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 2 0 1 1 1 1 1 0 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_onevsall.predict(X_test)\n",
    "print(\"Category for each instance:\")\n",
    "print(y_test)\n",
    "print(\"\\nPrediction:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9459\n"
     ]
    }
   ],
   "source": [
    "# Compute the Accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", round(accuracy,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
