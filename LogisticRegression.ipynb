{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absenteeism - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 740 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason for absence</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation expense</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Age</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>Hit target</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Absenteeism time in hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692552</td>\n",
       "      <td>0.724379</td>\n",
       "      <td>0.312360</td>\n",
       "      <td>0.337461</td>\n",
       "      <td>0.509511</td>\n",
       "      <td>0.463951</td>\n",
       "      <td>0.610497</td>\n",
       "      <td>0.647262</td>\n",
       "      <td>0.653058</td>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.360488</td>\n",
       "      <td>0.253737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.124304</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.378506</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>0.677741</td>\n",
       "      <td>0.127401</td>\n",
       "      <td>0.945206</td>\n",
       "      <td>0.872130</td>\n",
       "      <td>0.509911</td>\n",
       "      <td>0.354537</td>\n",
       "      <td>0.999076</td>\n",
       "      <td>0.432080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.369064</td>\n",
       "      <td>0.337820</td>\n",
       "      <td>0.083777</td>\n",
       "      <td>0.678456</td>\n",
       "      <td>0.703634</td>\n",
       "      <td>0.173419</td>\n",
       "      <td>0.744381</td>\n",
       "      <td>0.702182</td>\n",
       "      <td>0.920467</td>\n",
       "      <td>0.876997</td>\n",
       "      <td>0.690698</td>\n",
       "      <td>0.470167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.739270</td>\n",
       "      <td>0.646078</td>\n",
       "      <td>0.617184</td>\n",
       "      <td>0.416416</td>\n",
       "      <td>0.523913</td>\n",
       "      <td>0.448263</td>\n",
       "      <td>0.562862</td>\n",
       "      <td>0.681659</td>\n",
       "      <td>0.352872</td>\n",
       "      <td>0.360298</td>\n",
       "      <td>0.462425</td>\n",
       "      <td>0.249254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697562</td>\n",
       "      <td>0.725001</td>\n",
       "      <td>0.330455</td>\n",
       "      <td>0.338349</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>0.457796</td>\n",
       "      <td>0.604668</td>\n",
       "      <td>0.650673</td>\n",
       "      <td>0.657022</td>\n",
       "      <td>0.678083</td>\n",
       "      <td>0.361873</td>\n",
       "      <td>0.250986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Reason for absence  Month of absence  Day of the week   Seasons  \\\n",
       "0  0.692552            0.724379          0.312360         0.337461  0.509511   \n",
       "1  0.124304            0.088435          0.378506         0.954500  0.677741   \n",
       "2  0.369064            0.337820          0.083777         0.678456  0.703634   \n",
       "3  0.739270            0.646078          0.617184         0.416416  0.523913   \n",
       "4  0.697562            0.725001          0.330455         0.338349  0.561429   \n",
       "\n",
       "   Transportation expense  Distance from Residence to Work       Age  \\\n",
       "0                0.463951                         0.610497  0.647262   \n",
       "1                0.127401                         0.945206  0.872130   \n",
       "2                0.173419                         0.744381  0.702182   \n",
       "3                0.448263                         0.562862  0.681659   \n",
       "4                0.457796                         0.604668  0.650673   \n",
       "\n",
       "   Work load Average/day   Hit target  Disciplinary failure  Education  \\\n",
       "0                0.653058    0.690259              0.360488   0.253737   \n",
       "1                0.509911    0.354537              0.999076   0.432080   \n",
       "2                0.920467    0.876997              0.690698   0.470167   \n",
       "3                0.352872    0.360298              0.462425   0.249254   \n",
       "4                0.657022    0.678083              0.361873   0.250986   \n",
       "\n",
       "   Absenteeism time in hours  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from csv\n",
    "data_df = pd.read_csv('Absenteeism/Absenteeism_at_work_editted_continous_features_target_not_normalised.csv')\n",
    "\n",
    "print(\"Total:\",len(data_df),\"rows.\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly split data into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the dataframe and select 80% of the dataset for the training set and the remaining 20% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataframe\n",
    "data_df = data_df.sample(frac=1)\n",
    "\n",
    "# Split data into train and test set\n",
    "train_length = int(np.round(len(data_df) * 0.8))  # Train set: 80% of data\n",
    "test_length = len(data_df) - train_length         # Test set: remaining 20%\n",
    "train_df = data_df.head(train_length)\n",
    "test_df = data_df.tail(test_length)\n",
    "\n",
    "# Features and target (last column) of training set\n",
    "X_train = train_df.iloc[:,:-1].to_numpy()\n",
    "y_train = train_df.iloc[:,-1].to_numpy()\n",
    "\n",
    "# Features and target (last column) of test set\n",
    "X_test = test_df.iloc[:,:-1].to_numpy()\n",
    "y_test = test_df.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class for our logistic regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regressor Model\n",
    "class LogisticRegressor:\n",
    "    \n",
    "    # Initialise hyperparameters\n",
    "    def __init__(self, lr=0.01, max_iter=500, add_intercept=True):\n",
    "        self.learning_rate = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.add_intercept = add_intercept\n",
    "    \n",
    "    # Sigmoid function\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Cost function for gradient descent\n",
    "    def __cost_function(self, y, y_pred):\n",
    "        return (-y*log(y_pred) - ((1-y)*log(1-y_pred))).mean()\n",
    "    \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "        \n",
    "        # Initialise weights\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Keep a history of cost values\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Iterate\n",
    "        for i in range (self.max_iter):\n",
    "            \n",
    "            # prediction\n",
    "            y_pred = self.__sigmoid(np.dot(X, self.w.T))\n",
    "            \n",
    "            # cost\n",
    "            cost = self.__cost_function(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # gradient vector\n",
    "            gradient = np.dot(X.T, (y_pred - y)) / X.shape[1]\n",
    "            self.w -= gradient * self.learning_rate\n",
    "            \n",
    "    # Predict an output\n",
    "    def predict(self, X):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "        y_pred = self.__sigmoid(np.dot(X, self.w.T))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have three possible target classes (0, 0.5 and 1), we need to use One vs. All.\n",
    "To do so, we build a model for each class and for each instance, we choose the class for which the probability is highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, since we will have three models (one for each possible class), we need three target sets.\n",
    "In a \"one vs. rest\" perspective, for every output, if it is the \"one\" class, we set it to 1, if it is the \"rest\", we set it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1vsall = []\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# For each possible class\n",
    "for c in range (nb_classes):\n",
    "    y_one = np.where(y_train == c, 1, 0)\n",
    "    y_1vsall.append(y_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build a model for each possible class. Each model will have to compute the probability that the class of the instance is the \"one\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One Vs All Model\n",
    "class LogisticRegressorOneVsAll:\n",
    "    \n",
    "    # Initialise hyperparameters\n",
    "    def __init__(self, lr=0.01, max_iter=500, add_intercept=True):\n",
    "        self.learning_rate = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.add_intercept = add_intercept\n",
    "        \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        self.regressors = []\n",
    "\n",
    "        # For each \"one vs. all\" target set\n",
    "        for y_one in y:\n",
    "            # Build a model with target set\n",
    "            model = LogisticRegressor(lr=self.learning_rate,\n",
    "                                      max_iter=self.max_iter,\n",
    "                                      add_intercept=self.add_intercept)\n",
    "            # Train a model and add it to the list of regressors\n",
    "            model.fit(X, y_one)\n",
    "            self.regressors.append(model)\n",
    "        \n",
    "    # Predict an output\n",
    "    def predict(self, X):\n",
    "        final_pred = []\n",
    "        y_pred_1vsall = []\n",
    "\n",
    "        # For each regressor\n",
    "        for model in self.regressors:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_1vsall.append(y_pred)\n",
    "            \n",
    "        # For each instance\n",
    "        for i in range(len(X)):\n",
    "            best_pred = 0\n",
    "            best_target = -1\n",
    "            # Find the best prediction (model with highest probability)\n",
    "            for j in range(len(self.regressors)):\n",
    "                if y_pred_1vsall[j][i] > best_pred:\n",
    "                    best_pred = y_pred_1vsall[j][i]\n",
    "                    best_target = j\n",
    "            # Add best prediction to the final result\n",
    "            final_pred.append(best_target)\n",
    "\n",
    "        # Return final prediction\n",
    "        final_pred = np.asarray(final_pred)\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 100000\n",
    "\n",
    "# One vs. All regressor\n",
    "model_onevsall = LogisticRegressorOneVsAll(max_iter=nb_iterations)\n",
    "model_onevsall.fit(X_train, y_1vsall)\n",
    "y_pred = model_onevsall.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 2 1 2 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 2 2 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
